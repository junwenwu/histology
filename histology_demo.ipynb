{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eaf6aa10",
   "metadata": {},
   "source": [
    "# Histology Demo with Intel® Distribution of OpenVINO™ Toolkit. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4cededb",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "This sample requires the following:\n",
    "- All files are present and in the following directory structure:\n",
    "    \n",
    "    - **histology.ipynb** - This Jupyter* Notebook\n",
    "    - **data** - Directory to hold data\n",
    "    - **python** -Directory for the Python scripts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1addda2-a59a-4f78-96ea-5699dde13742",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "This demo uses the [colorectal histology images dataset](https://www.tensorflow.org/datasets/catalog/colorectal_histology) to train a simple convolutional neural network in TensorFlow and demonstrates how to use OpenVINO™ integration with Tensorflow and OpenVINO™ Inference Engine to do inference on different Hardware architecture.\n",
    "\n",
    "All images are RGB, 0.495 µm per pixel, digitized with an Aperio ScanScope (Aperio/Leica biosystems), magnification 20x. Histological samples are fully anonymized images of formalin-fixed paraffin-embedded human colorectal adenocarcinomas (primary tumors) from our pathology archive (Institute of Pathology, University Medical Center Mannheim, Heidelberg University, Mannheim, Germany).\n",
    "\n",
    "The model is trained using the histology model introduced here: [Kather JN, Weis CA, Bianconi F, Melchers SM, Schad LR, Gaiser T, Marx A, Zollner F: Multi-class texture analysis in colorectal cancer histology (2016), Scientific Reports (in press)] (https://zenodo.org/record/53169#.X_T3iC1h10v)\n",
    "\n",
    "\n",
    "\n",
    "### Key concepts\n",
    "This sample application includes an example for the following:\n",
    "- Application:\n",
    "  - Load and visualize Tensorflow dataset\n",
    "- Intel® DevCloud for the Edge: submitting jobs to perform on different edge compute nodes (rather than on the development node hosting this Jupyter* notebook)\n",
    "  - Training jobs that train a convolutional neural network with Tensorflow V2\n",
    "  - Running inference jobs with OpenVINO™ integration with Tensorflow\n",
    "  - Running Inference jobs with OpenVINO™ Inference Engine\n",
    "  - Monitoring job status\n",
    "  - Viewing results and assessing performance for hardware on different compute nodes\n",
    "- [Intel® Distribution of OpenVINO™ toolkit](https://software.intel.com/openvino-toolkit):\n",
    "  - Create the necessary Intermediate Representation (IR) files for the inference model using [Model Optimizer](http://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html)\n",
    "  - Run an inference application on multiple hardware devices using the [Inference Engine](http://docs.openvinotoolkit.org/latest/_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf0e1ad2-8fb6-4817-b00a-5973216c61e8",
   "metadata": {},
   "source": [
    "### Install necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b03e880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install -U pip==21.0.1\n",
    "!pip install importlib_resources\n",
    "!pip install openvino-dev==2021.4.2\n",
    "!pip install tensorflow==2.7.0\n",
    "!pip install openvino-tensorflow==1.1.0\n",
    "!pip install -U tensorflow_datasets\n",
    "!pip install ipywidgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4861efc0-56ad-447f-ad4b-dcbdef2dfffd",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aeace03",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import time\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0909535e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from platform import python_version\n",
    "print(python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21719f8e",
   "metadata": {},
   "source": [
    " ### Dataset Examples\n",
    " #### How to load TF colorrectal_histology dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce976a57-4097-4c05-bb75-e5174750b4d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['NO_GCE_CHECK'] = 'true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef3b42e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "(ds), ds_info =  tfds.load('colorectal_histology', data_dir=\".\", \n",
    "                                          shuffle_files=True, split='train', \n",
    "                                          with_info=True, as_supervised=True)\n",
    "\n",
    "assert isinstance(ds, tf.data.Dataset)\n",
    "print(ds_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10778d9",
   "metadata": {},
   "source": [
    "#### Display a few examples from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b29a38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_key, y_key = ds_info.supervised_keys\n",
    "ds_temp = ds.map(lambda x, y: {x_key: x, y_key: y})\n",
    "tfds.show_examples(ds_temp, ds_info, plot_scale=5);\n",
    "!mkdir test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8964b2",
   "metadata": {},
   "source": [
    "#### Check the histology classification subtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c45918a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_info.features['label'].names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57965c72",
   "metadata": {},
   "source": [
    "## Histology Demo\n",
    "The Histology demo uses the Intel® Optimized Tensorflow to perform training and inference on input color rectal histology image dataset using OpenVINO™ integration with Tensorflow and Intel® Distribution of OpenVINO™ toolkit .  We will setup, train, run, and view the results for this application for several different hardware available on the compute nodes within the Intel® DevCloud for the Edge.  To accomplish this, we will be performing the following tasks:\n",
    "\n",
    "1. Train the model in [Intel Optimized Tensorflow](https://software.intel.com/content/www/us/en/develop/tools/frameworks.html)\n",
    "1. Run Inference using [OpenVINO™ Integration with Tensorflow](https://www.intel.com/content/www/us/en/developer/tools/devcloud/edge/build/ovtfoverview.html)\n",
    "2. Use the [Model Optimizer](http://docs.openvinotoolkit.org/latest/_docs_MO_DG_Deep_Learning_Model_Optimizer_DevGuide.html) to create the inference model IR files needed to perform inference\n",
    "3. Create the job file used to submit running training and inference on compute nodes\n",
    "4. View results and assess performance \n",
    "\n",
    "### How it works\n",
    "At startup, for both training and inference, the Histology application configures itself by parsing the command line arguments. Once configured, the training application loads the specified input dataset, prepares the data accordingly and runs training on the specified edge compute node. Once the model is trained, the inference app loads the inference model's IR files into the [Inference Engine](http://docs.openvinotoolkit.org/latest/_docs_IE_DG_Deep_Learning_Inference_Engine_DevGuide.html) and runs inference on the specified input data to find the class of the given histology image. After the specific job is completed, the output is stored in the appropriate `results/<architecture>/` directory.  \n",
    "\n",
    "The following sections will guide you through configuring and running the Histology demo."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aceb7c97",
   "metadata": {},
   "source": [
    "### Model training for the histology images\n",
    "The following sections will go through the steps to run training on the current EC2 instance. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c08fcd0",
   "metadata": {},
   "source": [
    "#### Create the job file\n",
    "\n",
    "The job file is a [Bash](https://www.gnu.org/software/bash/) script that serves as a wrapper around the Python* executable of our application that can be executed directly on different types of edge compute node.  One purpose of the job file is to simplify running an application on different compute nodes by accepting a few arguments and then performing accordingly any necessary steps before and after running the application executable.  \n",
    "\n",
    "For this sample, the job file we will be using is already written for you and appears in the next cell.  The job file will be submitted as if it were run from the command line using the following format:\n",
    "```bash\n",
    "training_job.sh <output_directory> <input_directory> <epochs> <device> <num_cores>\n",
    "```\n",
    "Where the job file input arguments are:\n",
    "- <*output_directory*> - Output directory to use to store output files\n",
    "- <*input_directory*> - Path to the input training data\n",
    "- <*epochs*> - Epochs to train the model\n",
    "- <*device*> - Specify the device used for training\n",
    "- <*num_cores*> - If device is CPU, need to specify number of physical cores on the targeted device\n",
    "\n",
    "Based on the input arguments, the job file will do the following:\n",
    "- Change to the working directory `PBS_O_WORKDIR` where this Jupyter* Notebook and other files appear on the compute node\n",
    "- Create the <*output_directory*>\n",
    "- Run the application Python* executable with the appropriate command line arguments\n",
    "\n",
    "Run the following cell to create the `training_job.sh` job file.  The [`%%writefile`](https://ipython.readthedocs.io/en/stable/interactive/magics.html#cellmagic-writefile) line at the top will write the cell contents to the specified job file `training_job.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68915eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile training_job.sh\n",
    "\n",
    "# Store input arguments: <output_directory> <input_directory>\n",
    "INPUT_DIRECTORY=$1\n",
    "OUTPUT_DIRECTORY=$2\n",
    "EPOCHS=$3\n",
    "DEVICE=$4\n",
    "CORES=$5\n",
    "\n",
    "# The default path for the job is the user's home directory,\n",
    "#  change directory to where the files are.\n",
    "if [ ! -d \"./results/\" ];then\n",
    "   mkdir -p ./results\n",
    "fi\n",
    "# Make sure that the output directory exists.\n",
    "mkdir -p $OUTPUT_FILE\n",
    "\n",
    "# Install Tensorflow \n",
    "python python/run_training.py -i $INPUT_DIRECTORY \\\n",
    "                               -o $OUTPUT_DIRECTORY \\\n",
    "                               -e $EPOCHS \\\n",
    "                               -d $DEVICE \\\n",
    "                               -c $CORES\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e56e82",
   "metadata": {},
   "source": [
    "#### Submit training job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0e514ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "! bash ./training_job.sh colorectal_histology results/xeon/cascade_lake 25 CPU 96"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5052d729",
   "metadata": {},
   "source": [
    "#### Trained model\n",
    "Once the training job has completed, the queue system outputs the stdout and stderr streams, respecitvely.\n",
    "\n",
    "The generated model file is written to the directory `results/<device>/checkpoints` that was specified as the output directory to the job file.  \n",
    "\n",
    "<br><div class=danger><b>Wait!: </b>Please wait for the training job to complete before proceeding to the next step to do the OpenVINO™ model conversion and inferencing with OpenVINO™.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3de2e559",
   "metadata": {},
   "source": [
    "### Inference using OpenVINO™ Integration with Tensorflow and OpenVINO™ Inference Engine\n",
    "The following sections will go through the steps to run inference application for a single image using OpenVINO integration with Tensorflow and if dataset on OpenVINO Inference Engine. For viewing the results from job please refer to section 4.5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e889534",
   "metadata": {},
   "source": [
    "#### Inference job for single image using OpenVINO™ Integration with Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f396edcd",
   "metadata": {},
   "source": [
    "##### Create the job file\n",
    "Similarly to the training, we will run inference using OpenVINO™ integration with Tensorflow for single image.\n",
    "\n",
    "The inference job file we will be using is already written for you and appears in the next cell.  The job file will be submitted as if it were run from the command line using the following format:\n",
    "```bash\n",
    "run_tensorflow_openvino.sh <device> \n",
    "```\n",
    "\n",
    "Where the job file input arguments are:\n",
    "- <*device*> - Specify the device used for inferencing\n",
    "\n",
    "classification_ovtf_histology.py takes in multiple arguments as listed below\n",
    " - m \"histology_model\"\n",
    " - i \"input_layer of the model>\"\n",
    " - o \"input_layer of the model>\"\n",
    " - ip \"image in Jpeg format\"\n",
    " - l \"label map file\"\n",
    "                  \n",
    "\n",
    "Based on the input arguments, the job file will do the following:\n",
    "- Create the <*output_directory*>\n",
    "- Run the application Python* executable with the appropriate command line arguments\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c74c97c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile run_tensorflow_openvino.sh\n",
    "\n",
    "#!/bin/sh\n",
    "\n",
    "DEVICE=$1\n",
    "\n",
    "python python/classification_ovtf_histology.py -m \"./results/xeon/cascade_lake/frozen_histology.pb\" \\\n",
    "                       -i \"x\" \\\n",
    "                       -o \"Identity\" \\\n",
    "                       -ip \"./data/A8D0_CRC-Prim-HE-10_002c.tif_Row_1_Col_451.jpg\" \\\n",
    "                       -it \"image\" \\\n",
    "                       -l \"./data/labels_histology.txt\" \\\n",
    "                       -f \"openvino\" \\\n",
    "                       --input_height 150 \\\n",
    "                       --input_width 150 \\\n",
    "                       --input_mean 3 \\\n",
    "                       -d $DEVICE "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6d0ab1",
   "metadata": {},
   "source": [
    "##### Submit  Inference job using OpenVINO™ Integration with Tensorflow\n",
    " \n",
    "The inference workload will run on the CPU. It can be modified to schedule on another accelerator by passing the flag F as with a value of GPU, VPU, or VAD-M.\n",
    " \n",
    "The workload images are expected to be in JPEG format. Any custom inputs need to be converted to JPEG before performing inference on them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f03b90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!chmod +x ./run_tensorflow_openvino.sh\n",
    "! bash ./run_tensorflow_openvino.sh \"CPU\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aba5943",
   "metadata": {},
   "source": [
    "#### Convert the Tensorflow model to OpenVINO IR files\n",
    "This workload is run on the CPU."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15d6987f",
   "metadata": {},
   "source": [
    "Run the following cell for Model Optimizer to create the model IR files. It converts the saved model trained from an edge Xeon Cascade Lake node to OpenVINO™ model format and saves it under `models/ov/<Model Precision>`. The model conversion can use either the generated savedModel or the frozen tensorflow model. Below mo_convert.sh script supports both model formats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633a097a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_model=\"saved_model\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3d6853",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source mo_convert.sh $input_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e06d1ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_model=\"frozen_model\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b5d84ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "!source mo_convert.sh $input_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4125053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!echo \"\\nAll IR files that were created:\"\n",
    "!find ./models/ov/FP32 -name \"*.xml\" -o -name \"*.bin\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eac69f72",
   "metadata": {},
   "source": [
    "#### Run inference on Histology Dataset via OpenVINO™ Inference Engine\n",
    "The following sections will go through the steps to run our inference application for the Histology dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab6a9bf",
   "metadata": {},
   "source": [
    "##### Create the job file\n",
    "Similarly to the training, we will run inference on this EC2 instance node.\n",
    "\n",
    "The inference job file we will be using is already written for you and appears in the next cell.  The job file will be submitted as if it were run from the command line using the following format:\n",
    "```bash\n",
    "inference_job.sh <model_directory> <input_directory> <output_directory> <device>\n",
    "```\n",
    "Where the job file input arguments are:\n",
    "- <*model_directory*> - OpenVINO™ model directory\n",
    "- <*input_directory*> - Path to the input testing data\n",
    "- <*output_directory*> - Path to store the inference output\n",
    "- <*device*> - Specify the device used for inferencing\n",
    "\n",
    "Based on the input arguments, the job file will do the following:\n",
    "- Create the <*output_directory*>\n",
    "- Run the application Python* executable with the appropriate command line arguments\n",
    "\n",
    "Run the following cell to create the `inference_job.sh` job file.  The [`%%writefile`](https://ipython.readthedocs.io/en/stable/interactive/magics.html#cellmagic-writefile) line at the top will write the cell contents to the specified job file `inference_job.sh`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac1b086",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile inference_job.sh\n",
    "\n",
    "# Store input arguments: <model_type> <input_directory> <output_directory> <device>\n",
    "MODEL_TYPE=$1\n",
    "INPUT_DIRECTORY=$2\n",
    "OUTPUT_DIRECTORY=$3\n",
    "DEVICE=$4\n",
    "\n",
    "# Make sure that the output directory exists.\n",
    "mkdir -p $OUTPUT_DIRECTORY\n",
    "\n",
    "# Run the inference code\n",
    "python python/run_inference.py -d $DEVICE \\\n",
    "                                -i $INPUT_DIRECTORY \\\n",
    "                                -o $OUTPUT_DIRECTORY \\\n",
    "                                -m $MODEL_TYPE"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8bba9c",
   "metadata": {},
   "source": [
    "##### Run inference\n",
    "In the cell below, we run the inference workload ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "974b2e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "! chmod +x ./inference_job.sh\n",
    "! bash ./inference_job.sh FP32 test_data/testdata.npz results/xeon/cascade_lake/ CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb674db",
   "metadata": {},
   "outputs": [],
   "source": [
    "!find \"results/xeon/cascade_lake/\" -name \"stats.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cb0590",
   "metadata": {},
   "source": [
    "### View inferencing results\n",
    "\n",
    "The output classification results for each job is written to the file `stats_<*job id*>.txt` located in the directory `results/<device>` that was specified as the output directory to the job file. Each line of the result txt file records the following information:\n",
    "\n",
    "`index of the testing image, true class label, predicted class label`\n",
    "\n",
    "Example of how to view the inferencing results are shown below. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "217497cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#For viewing results\n",
    "output_file_clx = \"results/xeon/cascade_lake/\"+\"stats\"+\".txt\"\n",
    "fd = open( output_file_clx, 'r')\n",
    "print(fd.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2e8043b",
   "metadata": {},
   "source": [
    "## Quantization\n",
    "\n",
    "Quantization is the process of reducing the model's precision. By performing this optimization, you can accelerate your model execution time.  \n",
    "\n",
    "The [quantize.py](python/quantize.py) file contains quantization script and uses the [post training optimization toolkit (POT) API](https://docs.openvinotoolkit.org/latest/pot_compression_api_README.html) to reduce model's precision from FP32 to INT8. Quantization settings, such as the path to original model, path to dataset, quantization algorithm etc., which should be set via configs. ```DatasetsDataLoader``` creates quantization dataset from the sample video and loads one by one input images to POT, when quantization process starts. When quantization is finished, the INT8 model will be saved at ```'/models/ov/INT8'``` directory.\n",
    "\n",
    "Run the following cell to create the ```quantization_job.sh``` job file. This script runs quantization and benchmarking of the quantized and non-quantized models to compare their execution time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5e4dbad",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile quantization_job.sh\n",
    "\n",
    "# Run the quantization script\n",
    "python3.6 ./python/quantize.py\n",
    "\n",
    "# Run the benchmark_app for FP32 model\n",
    "benchmark_app \\\n",
    "        -m ./models/ov/FP32/saved_model.xml 2>/dev/null | grep Throughput | xargs echo FP32\n",
    "\n",
    "# Run the benchmark_app for INT8 model\n",
    "benchmark_app \\\n",
    "        -m ./models/ov/INT8/saved_model.xml 2>/dev/null | grep Throughput | xargs echo INT8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7a4a31c",
   "metadata": {},
   "source": [
    "Run the following cell to start quantization and benchmarking of the quantized model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4e20d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ee4b6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wait_for_job_to_finish(job_id):\n",
    "    if job_id:\n",
    "        print(\"Job submitted to the queue. Waiting for it to complete.\")    \n",
    "                    \n",
    "        # Reading the benchmark_app results\n",
    "        print('Results for FP32 and INT8 models:')\n",
    "        for line in job_id:            \n",
    "            if 'Throughput:' in line:\n",
    "                print(line.split('\\n')[0])\n",
    "                      \n",
    "    else:\n",
    "        print(\"Error in job submission.\")\n",
    "\n",
    "job_id_core = get_ipython().getoutput('bash ./quantization_job.sh')\n",
    "benchmarks = wait_for_job_to_finish(job_id_core)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
